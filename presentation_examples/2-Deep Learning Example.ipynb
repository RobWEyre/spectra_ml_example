{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "np.random.seed(12345)\n",
    "seaborn.set_context(\"talk\")\n",
    "seaborn.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Simple Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, num=2000)[:, np.newaxis]\n",
    "y = 1.5*x[:, 0]**3 - 5*x[:, 0]**2 + 3*x[:, 0] + 3*np.random.randn(x.shape[0])\n",
    "train_x = x[:1700]\n",
    "train_y = y[:1700]\n",
    "test_x = x[1700:]\n",
    "test_y = y[1700:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_x, train_y)\n",
    "plot(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape=(1,)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "history = model.fit(train_x, train_y, epochs=5, validation_split=0.4, batch_size=10, verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(train_x)\n",
    "test_pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_x, train_y)\n",
    "plot(test_x, test_y)\n",
    "plot(train_x, train_pred, 'r')\n",
    "plot(test_x, test_pred, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Example Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM models are best when there is a lot of autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t = np.linspace(0, 200, num=1000)\n",
    "x = np.random.randn(len(t)+1)\n",
    "y = 2.0*np.sin(t) + 1.0*np.random.randn(len(t))# + np.roll(x, 1)[1:]\n",
    "\n",
    "train_t = t[0:700]\n",
    "train_y = y[0:700]\n",
    "val_t = t[700:900]\n",
    "val_y = y[700:900]\n",
    "test_t = t[900:]\n",
    "test_y = y[900:]\n",
    "\n",
    "plot(t, y)\n",
    "\n",
    "plot(train_t, train_y)\n",
    "_ = plot(test_t, test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pd.plotting.autocorrelation_plot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data into sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM models require a sequence of previous timesteps to perform a forecast, so we need to transform the data. Luckily Keras has a built in function for doing this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "\n",
    "train_data = TimeseriesGenerator(train_y[:, np.newaxis], train_y,\n",
    "                               length=seq_len, sampling_rate=1,\n",
    "                               batch_size=10)\n",
    "val_data = TimeseriesGenerator(val_y[:, np.newaxis], val_y,\n",
    "                               length=seq_len, sampling_rate=1,\n",
    "                               batch_size=10)\n",
    "\n",
    "test_data = TimeseriesGenerator(test_y[:, np.newaxis], test_y,\n",
    "                               length=seq_len, sampling_rate=1,\n",
    "                               batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x, _y = train_data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y[0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_y[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras is a nice high-level wrapper on other packages (such as tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=[seq_len, 1]))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "history = model.fit_generator(train_data, epochs=8, validation_data=val_data, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the model is converging towards a good fit, you will see the validation error decrease and start to flatten out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history.history['val_loss'])\n",
    "xlabel('Epochs')\n",
    "_ = ylabel('Validation Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict future timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To do multi-step prediction, we feed in the previous predictions as the input to the next prediction. Errors will start to build up as we get further from the known data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "fdata = val_y[-seq_len:]\n",
    "for i in range(test_t.shape[0]):\n",
    "    pred = model.predict(fdata.reshape((1, seq_len, 1)))\n",
    "    preds.append(pred[0][0])\n",
    "    fdata = np.roll(fdata, -1)\n",
    "    fdata[-1] = pred\n",
    "\n",
    "plot(test_t, test_y)\n",
    "plot(test_t, preds)\n",
    "\n",
    "_ = legend(['actual', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
